{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXvbIVCkYtf_"
      },
      "source": [
        "# 1. Introduction – Let’s Customize Diffusion!\n",
        "\n",
        "Welcome to the Diffusion + LoRA Fine-Tuning Workshop! \n",
        "In this tutorial, we’ll show you how to fine-tune a powerful diffusion model using just a few images of yourself (or your cat, or your coffee mug—no judgment!).\n",
        "\n",
        "\n",
        "## What you’ll learn:\n",
        "\n",
        " How diffusion models work (briefly!)\n",
        "\n",
        " How to prepare your own dataset\n",
        "\n",
        " How to caption it like a pro\n",
        "\n",
        " How to fine-tune using LoRA\n",
        "\n",
        " And finally… see your AI clone generate magic\n",
        "\n",
        " Before We Begin: Upload 10–15 photos of yourself. Face visibility helps!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Credits:\n",
        "\n",
        "This code was adapted from: [Fine Tuning SDXL on a Free T4 Google Colab GPU](https://medium.com/@ravi.kaskuser/fine-tuning-sdxl-on-a-free-t4-google-colab-gpu-41ca2cd3cec8)\n",
        "\n",
        "All credits goes to [Ravi Adi Prakoso](https://medium.com/@ravi.kaskuser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpbIqlPxZBpK"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Install the required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ej1dH_xeD9J"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kN1qAqIedEr"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/diffusers.git@v0.32.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCYjMFEieoM7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P29hsFFKe2Zd"
      },
      "outputs": [],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpIQiuXnKzJj"
      },
      "source": [
        "# Captioning (optional)\n",
        "\n",
        "For diffusion models to “know” what they’re generating, they need text-image pairs. That’s where captioning comes in.\n",
        "\n",
        "We’ll assign a special token to represent your concept (e.g., \"a photo of TOK man\").\n",
        "Think of this as your model's personalized vocabulary word!\n",
        "\n",
        "Tip: Use consistent phrasing across captions. The simpler, the better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8gf5xh8e0zn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# load the processor and the captioning model\n",
        "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\",torch_dtype=torch.float16).to(device)\n",
        "\n",
        "# captioning utility\n",
        "def caption_images(input_image):\n",
        "    inputs = blip_processor(images=input_image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    pixel_values = inputs.pixel_values\n",
        "\n",
        "    generated_ids = blip_model.generate(pixel_values=pixel_values, max_length=50)\n",
        "    generated_caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return generated_caption\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7-heuBGfRlZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "# create a list of (Pil.Image, path) pairs\n",
        "local_dir = \"/content/drive/MyDrive/ICOICT_demo/images\"\n",
        "imgs_and_paths = [(path,Image.open(path)) for path in glob.glob(f\"{local_dir}*.jpg\")]\n",
        "\n",
        "imgs_and_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDBQzS7hfhGd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "caption_prefix = \"a photo of TOK man, \" #@param\n",
        "with open(f'{local_dir}metadata.jsonl', 'w') as outfile:\n",
        "  for img in imgs_and_paths:\n",
        "      caption = caption_prefix + caption_images(img[1]).split(\"\\n\")[0]\n",
        "      entry = {\"file_name\":img[0].split(\"/\")[-1], \"prompt\": caption}\n",
        "      json.dump(entry, outfile)\n",
        "      outfile.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5edTKWveaR7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DEhrsogfsEs"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# delete the BLIP pipelines and free up some memory\n",
        "del blip_processor, blip_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO6luyi9cOne"
      },
      "source": [
        "# Training – Time to Teach the Model\n",
        "\n",
        "Now the fun part: let’s fine-tune the model using LoRA — a lightweight way to inject new knowledge into a huge model without retraining the whole thing.\n",
        "\n",
        "\n",
        "What’s Happening Under the Hood:\n",
        "\n",
        "  LoRA adds a few trainable adapters to the model\n",
        "\n",
        "  It’s fast, cheap, and doesn’t mess with the core weights!\n",
        "\n",
        "  Expect ~30–90 minutes training time (depending on settings & hardware)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkXZBnJ7uu1o"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes transformers accelerate peft -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbsywygm0s7q"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!accelerate config default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Dreambooth LoRA training script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGOwfaXGuz06"
      },
      "outputs": [],
      "source": [
        "\n",
        "!wget https://raw.githubusercontent.com/huggingface/diffusers/v0.32.1/examples/dreambooth/train_dreambooth_lora_sdxl.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1BUKHLmZ0di"
      },
      "source": [
        "Adjust your parameters\n",
        "\n",
        "\n",
        "*   instance_data_dir: location of your images\n",
        "*   output_dir: the model will be saved here\n",
        "*   resolution: try different resolutions (256, 512, 1024), the higher resolution the longer training time\n",
        "* instance prompt: keyword associated with your photo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avhKbQY_f0Ni"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env bash\n",
        "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
        "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
        "  --instance_data_dir=\"/content/drive/MyDrive/ICOICT_demo/images\" \\\n",
        "  --output_dir=\"model_LoRA\" \\\n",
        "  --resolution=256 \\\n",
        "  --instance_prompt=\"a photo of TOK man\" \\\n",
        "  --caption_column=\"prompt\"\\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=3 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --snr_gamma=5.0 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --max_train_steps=600 \\\n",
        "  --checkpointing_steps=100 \\\n",
        "  --seed=\"0\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I6hvXtOcJXg"
      },
      "source": [
        "# INFERENCE\n",
        "\n",
        "Your model is now trained. Let’s put it to the test!\n",
        "\n",
        "Try generating images with creative prompts using your special token:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6W2DqMwZkOq"
      },
      "source": [
        "### Load the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4N97bspy187"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import DiffusionPipeline, AutoencoderKL\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    vae=vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True\n",
        ")\n",
        "_ = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DSOY-_SE0Vj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/ICOICT_demo/output_images\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def save_image_incremental(image, output_dir, prefix=\"image\", ext=\".jpg\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # List files with the given prefix and extension\n",
        "    existing_files = [f for f in os.listdir(output_dir) if f.startswith(prefix) and f.endswith(ext)]\n",
        "\n",
        "    # Extract numbers from filenames\n",
        "    existing_nums = []\n",
        "    for f in existing_files:\n",
        "        try:\n",
        "            num = int(f.replace(prefix + \"-\", \"\").replace(ext, \"\"))\n",
        "            existing_nums.append(num)\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    # Get the next number\n",
        "    next_num = max(existing_nums, default=0) + 1\n",
        "    filename = f\"{prefix}-{next_num}{ext}\"\n",
        "\n",
        "    # Save the image\n",
        "    image.save(os.path.join(output_dir, filename))\n",
        "    print(f\"Saved: {filename}\")\n",
        "    return filename\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oIuh9eTZo6E"
      },
      "source": [
        "## Load your own fine-tuned  model\n",
        "\n",
        "If you haven't download your fine-tuned model and upload it to the specified directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdLBHcSYzIuy"
      },
      "outputs": [],
      "source": [
        "repo_id = \"/content/drive/MyDrive/ICOICT_demo/models/pytorch_lora_weights-600.safetensors\"\n",
        "pipe.load_lora_weights(repo_id)\n",
        "_ = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8oYcMqYZuFZ"
      },
      "source": [
        "## Generate your images\n",
        "\n",
        "Experiment with different prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXZo-PgnzRBy"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "trigger = \"a photo of TOK man, \"\n",
        "\n",
        "# Example prompts for generating images\n",
        "example_prompts = [\n",
        "    \"futuristic cyberpunk style, glowing neon lights in the background, blue and pink lighting, wearing a cyber visor, moody expression, rain falling, ultra-detailed face, stylized realism\",\n",
        "    \"futuristic cyberpunk style, glowing neon lights in the background, blue and pink lighting, wearing a cyber visor, moody expression, rain falling, ultra-detailed face, stylized realism\",\n",
        "    \"hiking through a misty forest, wearing a hooded jacket, slight rain on his face, atmospheric background, realistic style, cinematic mood, sharp face details\",\n",
        "    \"standing confidently at the head of a meeting table, arms crossed, business casual attire, glass-walled office background, leadership presence, natural lighting, high-resolution detail\",\n",
        "    \"smiling confidently against a white studio background, wearing a light blue shirt and blazer, well-lit for media and marketing use, modern corporate photography style, photorealistic\",\n",
        "    \"laughing and engaging with coworkers in a creative office space, casual but polished look, natural interaction, modern workplace environment, clear facial detail, warm and friendly tone\",\n",
        "    \"standing under colorful neon signs in a busy night market, lively background blur, side lighting casting soft shadows, reflective surfaces, vibrant city energy, highly detailed portrait\",\n",
        "    \"standing under a temple in Japan, lively background blur, daylight, reflective surfaces, vibrant city energy, highly detailed portrait\",\n",
        "    \"standing in a desert at sunset, windswept scarf, rugged face with sun-kissed skin, dramatic sky in the background, realistic shadows, adventure mood, cinematic detail\",\n",
        "    \"working at a desk in a bright open-plan office, laptop in front of him, natural daylight from large windows, smart-casual outfit, candid professional moment, realistic lighting and detail\",\n",
        "    \"looking out a bright  window, soft indoor lighting, glossy glass in focus, smile, bright background, ultra-realistic lighting\",\n",
        "    \"professional studio headshot, plain dark background, soft diffused lighting, confident expression, detailed facial features, symmetrical composition, ultra-realistic skin texture\",\n",
        "    \"looking out a rainy window, soft indoor lighting, raindrops on glass in focus, melancholic expression, warm cozy background, ultra-realistic lighting, shallow depth of field\",\n",
        "    \"wearing a navy business suit and white shirt, standing in front of a modern office backdrop, confident and approachable expression, clean lighting, realistic skin texture, professional portrait style\",\n",
        "    \"standing on a rooftop at sunset, cinematic lighting, wearing a black leather jacket, bokeh city lights in the background, shallow depth of field, dramatic sky, realistic style, high detail, wide angle\",\n",
        "]\n",
        "\n",
        "# Generate a random prompt using the trigger and an example prompt\n",
        "prompt = f\"{trigger} {random.choice(example_prompts)}\"\n",
        "prompt\n",
        "\n",
        "# Generate an image using the prompt and save\n",
        "image = pipe(prompt=prompt, num_inference_steps=25).images[0]\n",
        "save_image_incremental(image, output_dir)\n",
        "image\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SpbIqlPxZBpK",
        "3I6hvXtOcJXg"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tugas3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
